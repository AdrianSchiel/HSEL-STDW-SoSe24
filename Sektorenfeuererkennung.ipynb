{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b075b49c451deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pysrt\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be81121210767ba",
   "metadata": {},
   "source": [
    "# Prüfung der Sektorgrenzen von Sektorenfeuern mittels Drohnenaufnahmen\n",
    "\n",
    "## Einleitung\n",
    "\n",
    "Das folgende Jupyter Notebook beschäftigt sich mit der Überprüfung der Sektorgrenzen von Sektorenfeuern anhand von Drohnenaufnahmen. Bei Sektorenfeuern handelt es sich um Seeschifffahrtszeichen, mit unterschiedlichen Kennungen oder Farben. So markiert ein Leitfeuer wie aus Abbildung 1 mittels des weißen Sektors die optimale Fahrlinie im Fahrwasser, während der Rote und Grüne Sektor die Notwendigkeit zum Korrigieren des Kurses aufzeigen.[[1]](https://doi.org/10.24053/9783739882161)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./abbildungen/leitfeuer.png\" width=\"700\">\n",
    "<br>\n",
    "Abbildung 1: Leitfeuer \n",
    "</p>\n",
    "\n",
    "Diese Sektorenfeuer sind in einigen Fällen auf eine hohe Präzision angewiesen, da diese über eine hohe Tragweite verfügen und bereits geringe Abweichungen in einer entsprechend großen Entfernung, wie beispielsweise 30 Seemeilen im Falle des Leuchtturms Campen, große Auswirkungen haben.[[2]](https://www.deutsche-leuchtfeuer.de/nordsee/campen.html)\n",
    "\n",
    "Für die Erarbeitung von Lösungswegen für eine solche Überprüfung werden in diesem Notebook Drohnenaufnahmen des Leuchtfeuers Wybelsum verwendet. Bei diesem Handelt es sich um ein Quermarkenfeuer mit den Sektoren W 295°-320°, R -024°, W -049° an der Position N53°20'10\" E07°06'20\" mit einer Feuerhöhe von 16 m.[[3]](https://www.deutsche-leuchtfeuer.de/binnen/ems/wybelsum.html) In Abbildung 2 ist ein Kartenausschnitt des Leuchtfeuers und dessen Sektorgrenzen zu sehen.[[4]](https://map.openseamap.org/)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./abbildungen/wybelsum.png\" width=\"700\">\n",
    "<br>\n",
    "Abbildung 2: Quermarkenfeuer Wybelsum\n",
    "</p>\n",
    "\n",
    "Die zur Verfügung gestellten Aufnahmen bestehen aus mehreren Abflügen des Sektorenfeuers bei Nacht, welche sich in Höhe und Entfernung zum Feuer unterscheiden. Die zu den Aufnahmen gehörenden Metadaten liegen als Untertitel in den `.SRT` Dateien bei. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20115d0957f39021",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data/Flug_1.MP4 https://mux.hs-emden-leer.de/stdaw/20240613_nachtaufnahmen_sektorenfeuer/DJI_20240613232223_0002_V.MP4\n",
    "!wget -O data/Flug_1.SRT https://mux.hs-emden-leer.de/stdaw/20240613_nachtaufnahmen_sektorenfeuer/DJI_20240613232223_0002_V.SRT\n",
    "\n",
    "!wget -O data/Flug_2.MP4 https://mux.hs-emden-leer.de/stdaw/20240613_nachtaufnahmen_sektorenfeuer/DJI_20240613232811_0003_V.MP4\n",
    "!wget -O data/Flug_2.SRT https://mux.hs-emden-leer.de/stdaw/20240613_nachtaufnahmen_sektorenfeuer/DJI_20240613232811_0003_V.SRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0b60a88554b95",
   "metadata": {},
   "source": [
    "\n",
    "### Projektrahmen\n",
    "\n",
    "Das Projekt wird im Rahmen des Moduls \"Spezielle Themen der Datenwissenschaft\" bei Prof. Carsten Koch an der Hochschule Emden-Leer durchgeführt. Die Idee des Projektes kam dabei von dem Wasserstraßen- und Schifffahrtsamt in Emden. Die Umsetzung erfolgt durch die Studenten Adrian Schiel(7022935), Mirko Labitzke(7021691) und Tarek Harms(7022221) im Sommersemester 2024. Das Projekt wird neben Prof. Carsten Koch noch von den studentischen Mitarbeitern Tilman Leune und Malte Czesnik betreut. Die Drohnenaufnahmen wurden ebenfalls von diesen aufgenommen und bereitgestellt.\n",
    "\n",
    "## Analyse\n",
    "\n",
    "Die Analyse zu diesem Projekt besteht aus der Sichtung der Drohnenaufnahmen sowie der Recherche nach weiterer Literatur, in denen eine ähnliche Problematik behandelt wird. Zu den für die Recherche verwendeten Schlagworten gehörten \"visual\", \"UAV\" (unmanned aerial vehicle) und \"ATON\" (aids to navigation). Unter diesen Schlagworten lassen sich allerdings keine Ergebnisse finden, welche für dieses Projekt gewinnbringend sind. Aus dem Bereich der Ampelerkennung lässt sich das Paper \"Traffic light detection with color and edge information\"[[5]](https://ieeexplore.ieee.org/document/5234518) finden, in dem die Erkennung von Ampellichtern anhand der Farbe und Form realisiert wird.  \n",
    "Bei der Sichtung der Videoaufnahmen ist zu erkennen, dass der sich im Hintergrund befindliche Emder Hafen viele weitere Lichtquellen ins Bild bringt. Für eine bessere Handhabe der Aufnahmen wird diese zunächst in kleinere Ausschnitte aufgeteilt. Hierbei werden 5 Sekunden lange Ausschnitte des Videos und Untertitels in denen ein Wechsel der Sektoren auftritt manuell mithilfe des Kommandozeilenwerkzeugs `ffmpeg` ausgeschnitten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c427cb260502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i data/Flug_1.MP4 -ss 00:00:15 -to 00:00:20 -c copy data/Flug_1_part_1.MP4\n",
    "!ffmpeg -i data/Flug_1.SRT -ss 00:00:15 -to 00:00:20 -c copy data/Flug_1_part_1.SRT\n",
    "\n",
    "!ffmpeg -i data/Flug_1.MP4 -ss 00:00:43 -to 00:00:48 -c copy data/Flug_1_part_2.MP4\n",
    "!ffmpeg -i data/Flug_1.SRT -ss 00:00:43 -to 00:00:48 -c copy data/Flug_1_part_2.SRT\n",
    "\n",
    "!ffmpeg -i data/Flug_1.MP4 -ss 00:01:38 -to 00:01:43 -c copy data/Flug_1_part_3.MP4\n",
    "!ffmpeg -i data/Flug_1.SRT -ss 00:01:38 -to 00:01:43 -c copy data/Flug_1_part_3.SRT\n",
    "\n",
    "!ffmpeg -i data/Flug_1.MP4 -ss 00:02:57 -to 00:03:02 -c copy data/Flug_1_part_4.MP4\n",
    "!ffmpeg -i data/Flug_1.SRT -ss 00:02:57 -to 00:03:02 -c copy data/Flug_1_part_4.SRT\n",
    "\n",
    "!ffmpeg -i data/Flug_1.MP4 -ss 00:03:43 -to 00:03:48 -c copy data/Flug_1_part_5.MP4\n",
    "!ffmpeg -i data/Flug_1.SRT -ss 00:03:43 -to 00:03:48 -c copy data/Flug_1_part_5.SRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b84ed2d596de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i data/Flug_2.MP4 -ss 00:00:01 -to 00:00:06 -c copy data/Flug_2_part_1.MP4\n",
    "!ffmpeg -i data/Flug_2.SRT -ss 00:00:01 -to 00:00:06 -c copy data/Flug_2_part_1.SRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093ce9961a95c62",
   "metadata": {},
   "source": [
    "### Verwendete Bibliotheken\n",
    "\n",
    "- `cv2`: OpenCV, eine Bibliothek zur Echtzeit-Computer-Vision. (`4.10.0.84`) \n",
    "- `matplotlib.pyplot`: Eine Bibliothek für die Erstellung von Diagrammen und Visualisierungen.\n",
    "- `numpy`: Eine Bibliothek zur numerischen Berechnung in Python. (`1.26.4`)\n",
    "- `pysrt`: Eine Bibliothek zum Lesen, Schreiben und Bearbeiten von `.srt`-Untertiteldateien. (`1.1.2`)\n",
    "- `geopy`: Eine Bibliothek zur Berechnung von Entfernungen zwischen zwei Punkten angegeben in Längen- und Breitengraden. (`2.4.1`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270fdd0f2bcfb02",
   "metadata": {},
   "source": [
    "### Verwendete Pfade\n",
    "\n",
    "- `video_path`: Pfad zum Eingabevideo, welches verarbeitet bzw. analysiert werden soll\n",
    "- `srt_path`: Pfad zur passenden Untertiteldatei zu7m Eingabevideo im .srt-Format\n",
    "- `output_video`:   Pfad zum Ausgabevideo des Jupiter-Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2141b83e18a1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'data/Flug_1_part_1.MP4'\n",
    "srt_path = 'data/Flug_1_part_1.SRT'\n",
    "output_video = 'output_video_with_subtitles_part_1_hsv.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e801b3",
   "metadata": {},
   "source": [
    "## Funktion: `get_all_frames`\n",
    "\n",
    "Diese Funktion lädt alle Frames des Eingabevideos und speichert sie als RGB-Bilder in einem Array.\n",
    "\n",
    "### Ablauf:\n",
    "1. **Video einlesen**: Öffnet das Video und ermittelt die Eigenschaften wie Breite, Höhe und Anzahl der Frames.\n",
    "2. **Frames speichern**: Liest jedes Frame ein, konvertiert es in den RGB-Farbraum und speichert es in einem Array (`buf`).\n",
    "3. **Ressourcen freigeben**: Schließt das Video nach dem Einlesen aller Frames.\n",
    "4. **Ergebnis**: Gibt das Array mit den konvertierten Frames zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911add67944a2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    buf = np.empty((frameCount, frameHeight, frameWidth, 3), np.dtype('uint8'))\n",
    "\n",
    "    fc = 0\n",
    "    ret = True\n",
    "\n",
    "    while (fc < frameCount  and ret):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        buf[fc] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        fc += 1\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0038281947c4c2",
   "metadata": {},
   "source": [
    "Das Eingabevideo wird eingelesen, danach wird das erste und das letzte Bild des Arrays angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba9d53c37555a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_all_frames(video_path)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].imshow(frames[0])\n",
    "axs[1].imshow(frames[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6e15223f4e913",
   "metadata": {},
   "source": [
    "Die Bilder veranschaulichen die Änderung während der Aufnahme, dabei befindet sich das Sektorenfeuer im gesamten Ausschnitt ungefähr mittig, weiter links und rechts im Bild auf etwa gleicher Höhe befinden sich weitere Lichtquellen, die auf den ersten Blick eine ähnliche Farbe wir das weiße Sektorenfeuer aufweisen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffba72bb0d4697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hsv(frame_hsv, fig, pos, channel, label, cmap='hsv'):\n",
    "    ax = fig.add_subplot(pos, projection='3d')\n",
    "    H = frame_hsv[:, :, channel]    \n",
    "    X, Y = np.meshgrid(range(frame_hsv.shape[1]), range(frame_hsv.shape[0]))\n",
    "    ax.plot_surface(X, Y, H, cmap=cmap)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_box_aspect(aspect=None, zoom=0.8)\n",
    "    ax.set_zlabel(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2831c67c37f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_cropped = frames[:, 1175:1200, 1650:1900, :]\n",
    "\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "\n",
    "axs1 = fig.add_subplot(241)\n",
    "axs2 = fig.add_subplot(245)\n",
    "axs1.imshow(frames_cropped[82])\n",
    "axs2.imshow(frames_cropped[85])\n",
    "\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[82], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 242, 0, \"Hue\")\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[85], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 246, 0, \"Hue\")\n",
    "\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[82], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 243, 1, \"Saturation\", cmap='hot')\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[85], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 247, 1, \"Saturation\", cmap='hot')\n",
    "\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[82], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 244, 2, \"Value\", cmap='hot')\n",
    "frame_hsv = cv2.cvtColor(frames_cropped[85], cv2.COLOR_RGB2HSV)\n",
    "plot_hsv(frame_hsv, fig, 248, 2, \"Value\", cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4bc8dffca5084",
   "metadata": {},
   "source": [
    "Die hier gezeigten Abbildungen zeigen 3D Plots der einzelnen HSV-Kanäle. Hierbei wird bewusst ein schmaler Streifen aus dem Bild verwendet, um somit den Rand des Feuers besser zu erkennen. Während das weiße Feuer einen klar zu unterscheidenden Hue wert gegenüber dem Hintergrund aufweist, geht dieser beim roten Feuer im Rauschen etwas unter. Ebenso nimmt das Zentrum des roten Feuers ähnliche H Werte wie das weiße Feuer an. In der Helligkeit und Sättigung hebt sich das Feuer gegenüber dem benachbarten Hintergrund ab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddabb6c9dcaa7354",
   "metadata": {},
   "source": [
    "\n",
    "## Konzept\n",
    "\n",
    "Die Erkennung und Prüfung des Sektorenfeuers wird in drei Schritte unterteilt. Zunächst wird die Position des Feuers auf dem Video ausgemacht, um den weiterzuverarbeitenden Ausschnitt des Videos einzugrenzen. Es folgt eine Erkennung der Sektorgrenze anhand der aufeinanderfolgenden Bildausschnitte. Zuletzt wird anhand der Position des Leuchtturms und der Position der Drone berechnet, wie weit die Sektorgrenze vom vorgegeben Kurs abweicht. \n",
    "\n",
    "\n",
    "### Position des Leuchtfeuers\n",
    "\n",
    "Um die genaue Position des Leuchtfeuers innerhalb eines Videos zu ermitteln, ist es entscheidend, die relevanten Bildbereiche präzise zu identifizieren und zu analysieren. Für diese Identifizierung der Bildbereiche sind zwei Vorgehensweisen vorgesehen. Die erste orientiert sich an dem Vorgehen aus dem Paper Traffic light detection with color and edge information[[5]](https://ieeexplore.ieee.org/document/5234518). In diesem werden die RGB-Werte eines Frames normalisiert und anhand von Filtern das Leuchtsignal hervorgehoben. \n",
    "Das zweite Vorgehen ähnelt dem aus dem Paper, allerdings werden hier im HSV-Farbraum gefiltert. Dazu werden die einzelnen Frames des Videos untersucht, um potenzielle Lichtquellen anhand ihrer charakteristischen Farben zu erkennen. Diese Farben, in diesem Fall im roten und weißen Spektrum, werden durch entsprechende Farbmasken im HSV-Farbraum hervorgehoben. Durch die Ermittlung der Konturen der identifizierten Farbbereiche und die Bestimmung der größten zusammenhängenden Objekte lassen sich die Position und Ausdehnung des Feuers in jedem Frame des Videos in Form von Bounding Boxes präzise festhalten.\n",
    "\n",
    "### Bestimmung der Sektorgrenze\n",
    "\n",
    "Die Bestimmung der Sektorgrenze erfolgt anhand der für den Bildausschnitt berechneten Durchschnittswerte der Farbkanäle der einzelnen Frames. Für die einzelnen Kanäle wird geprüft, ob und wie deutlich eine Sektorgrenze zu erkennen ist. Hierzu werden die einzelnen Kanäle der Farbräume RGB und HSV verwendet. \n",
    "\n",
    "\n",
    "### Prüfen des Kurses\n",
    "\n",
    "Die Prüfung des Kurses der Sektorgrenze erfolgt mittels der Position der Drohne an der ermittelten Sektorgrenze und der Position des Feuers. Anhand dieser Parameter wird der Kurs von der Drohne zum Feuer berechnet. Dieser berechnete Kurs wird daraufhin mit dem Soll-Wert verglichen. \n",
    "\n",
    "## Umsetzung\n",
    "\n",
    "### Normalisieren und filtern im RGB-Farbraum\n",
    "\n",
    "Das Verfahren aus dem erwähnten Paper sieht vor zunächst die Farbwerte des Frames zu normalisieren. Durch diese Normalisierung wird eine bessere Vergleichbarkeit der Farben untereinander erzielt[[7]](https://www.sciencedirect.com/science/article/pii/S0031320316301510). Diese Normalisierung wird in der Funktion `normalize_rgb_frame` implementiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ab03dc747cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rgb_frame(frame_rgb):\n",
    "    frame_rgb = np.array(frame_rgb)    \n",
    "    frame_normalized_rgb = np.zeros_like(frame_rgb)\n",
    "    sums = np.sum(frame_rgb[:, :], axis=2)\n",
    "\n",
    "    frame_normalized_rgb[:, :, 0] = frame_rgb[:, :, 0] / sums * 255\n",
    "    frame_normalized_rgb[:, :, 1] = frame_rgb[:, :, 1] / sums * 255\n",
    "    frame_normalized_rgb[:, :, 2] = frame_rgb[:, :, 2] / sums * 255\n",
    "    \n",
    "    return frame_normalized_rgb\n",
    "\n",
    "\n",
    "normalized_frame_first = normalize_rgb_frame(frames[0])\n",
    "normalized_frame_last = normalize_rgb_frame(frames[-1])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].imshow(normalized_frame_first)\n",
    "axs[1].imshow(normalized_frame_last)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6979cf7c780af2",
   "metadata": {},
   "source": [
    "Die obere Abbildung zeigt den ersten und letzten Frame mit normalisierten RGB-Werten. Der im ersten Frame zu sehende rote Sektor hebt sich vom Hintergrund ab. Der im letzten Frame zu sehende weiße Sektor hingegen weniger deutlich. Nach der Normalisierung werden mittels der Funktion `get_filter_frame` Filter erstellt. Die Grenzwerte der Filter werden zunächst aus dem Paper übernommen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707bc195234141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_frame(normalized_frame):    \n",
    "    filter_R_1 = normalized_frame[:, :, 0] > 200\n",
    "    filter_G_1 = normalized_frame[:, :, 1] < 150\n",
    "    filter_B_1 = normalized_frame[:, :, 2] < 150\n",
    "    \n",
    "    filter_1 = filter_R_1 & filter_G_1 & filter_B_1\n",
    "\n",
    "    filter_R_2 = normalized_frame[:, :, 0] > 200\n",
    "    filter_G_2 = normalized_frame[:, :, 1] > 150\n",
    "    filter_B_2 = normalized_frame[:, :, 2] < 150\n",
    "    \n",
    "    filter_2 = filter_R_2 & filter_G_2 & filter_B_2\n",
    "    \n",
    "    filter_R_3 = normalized_frame[:, :, 0] < 150\n",
    "    filter_G_3 = normalized_frame[:, :, 1] > 240\n",
    "    filter_B_3 = normalized_frame[:, :, 2] > 220\n",
    "    \n",
    "    filter_3 = filter_R_3 & filter_G_3 & filter_B_3\n",
    "    \n",
    "    return filter_1 | filter_2 | filter_3\n",
    "\n",
    "\n",
    "filter_frame_first = get_filter_frame(normalized_frame_first)\n",
    "filter_frame_last = get_filter_frame(normalized_frame_last)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].imshow(filter_frame_first)\n",
    "axs[1].imshow(filter_frame_last)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caae83f797578ab",
   "metadata": {},
   "source": [
    "Die Abbildungen zeigen, dass der aus dem Paper übernommene Filter sich nicht nahtlos auf die Problemstellung dieser Arbeit anwenden lässt, weshalb die Filter im folgenden etwas angepasst werden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb4812ed2e1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_frame(normalized_frame):\n",
    "    filter_R_1 = normalized_frame[:, :, 0] > 125\n",
    "    filter_G_1 = normalized_frame[:, :, 1] < 200\n",
    "    filter_B_1 = normalized_frame[:, :, 2] < 200\n",
    "\n",
    "    filter_1 = filter_R_1 & filter_G_1 & filter_B_1\n",
    "\n",
    "    filter_R_2 = normalized_frame[:, :, 0] > 100\n",
    "    filter_G_2 = normalized_frame[:, :, 1] > 75\n",
    "    filter_B_2 = normalized_frame[:, :, 2] < 150\n",
    "\n",
    "    filter_2 = filter_R_2 & filter_G_2 & filter_B_2\n",
    "\n",
    "    filter_R_3 = normalized_frame[:, :, 0] < 150\n",
    "    filter_G_3 = normalized_frame[:, :, 1] > 240\n",
    "    filter_B_3 = normalized_frame[:, :, 2] > 220\n",
    "\n",
    "    filter_3 = filter_R_3 & filter_G_3 & filter_B_3\n",
    "\n",
    "    return filter_1 | filter_2 | filter_3\n",
    "\n",
    "\n",
    "filter_frame_first = get_filter_frame(normalized_frame_first)\n",
    "filter_frame_last = get_filter_frame(normalized_frame_last)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].imshow(filter_frame_first)\n",
    "axs[1].imshow(filter_frame_last)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e527c68bbdbb3",
   "metadata": {},
   "source": [
    "Durch die Anpassung der filter hebt sich sowohl der rote als auch der weiße Sektor deutlich ab. Die Filter weisen nun allerdings eine solch hohe Empfindlichkeit auf, sodass auch das Hintergrundrauschen sichtbarer wird. Die Normalisierung und der Filter werden in der im folgenden erläuterten Funktion `get_Bounding_Boxes` angewendet.  \n",
    "\n",
    "\n",
    "### Funktion: `get_Bounding_Boxes`\n",
    "\n",
    "Diese Funktion versucht in dem Eingabevideo den Leuchtturm zu finden und passend um diesen für jeden Frame in dem Video ein Rechteck zu generieren. Das Array bounding_boxes wird zurückgeben und beinhaltet für jeden Frame im Video ein Koordinatenpaar für das jeweilige Rechteck, sowie die Höhe und Breite. Die Funktion sucht primär nach roten und weißen Flächen, dies müsste für Leuchttürme angepasst werden, bei denen andere Farben relevant sind.\n",
    "\n",
    "#### Ablauf:\n",
    "1. **Video laden**: Öffnet das Video und initialisiert eine Liste zur Speicherung der Bounding Boxes.\n",
    "2. **Frames verarbeiten**: Jedes Frame wird auf den zentralen Bereich zugeschnitten und in den HSV-Farbraum konvertiert.\n",
    "3. **Farberkennung**: Normalisierung des Frames und Erstellung der Maske\n",
    "4. **Konturen finden**: Die größte Kontur in jedem Frame wird erkannt, und ihre Bounding Box wird gespeichert.\n",
    "5. **Ergebnis**: Gibt eine Liste der Bounding Boxes für jedes Frame zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82003ddc2d6eaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bounding_Boxes(video_path):\n",
    "    # Video öffnen\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Liste zur Speicherung der Bounding Box Positionen\n",
    "    bounding_boxes = []\n",
    "\n",
    "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fc = 0\n",
    "    ret = True\n",
    "    while (fc < frameCount  and ret):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Frame in den mittleren Bereich zuschneiden\n",
    "        height, width, _ = frame.shape\n",
    "        center_frame = frame[height//4: 3*height//4, width//4: 3*width//4]\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(center_frame, cv2.COLOR_BGR2RGB)\n",
    "        normalized_rgb_frame = normalize_rgb_frame(rgb_frame)\n",
    "        filtermaske = get_filter_frame(normalized_rgb_frame)\n",
    "        filtermaske = (filtermaske.astype(np.uint8)) * 255\n",
    "\n",
    "        # Konturen finden\n",
    "        contours, _ = cv2.findContours(filtermaske, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Größtes Objekt finden und Position speichern\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            bounding_boxes.append([x + width//4, y + height//4, w, h])\n",
    "\n",
    "        fc += 1\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fe7237f8833fa",
   "metadata": {},
   "source": [
    "### Funktion: `get_subtitle_for_time`\n",
    "\n",
    "Diese Funktion sucht die passenden Metadaten für einen bestimmten Zeitpunkt im Video.\n",
    "\n",
    "### Ablauf:\n",
    "1. **Zeiten berechnen**: Die Start- und Endzeit jedes Untertitels werden in Sekunden umgerechnet.\n",
    "2. **Untertitel finden**: Es wird überprüft, ob der gegebene Zeitpunkt innerhalb der Zeitspanne eines Untertitels liegt.\n",
    "3. **Ergebnis**: Gibt den Text die entsprechenden Metadaten zurück oder `None`, wenn kein Untertitel für diesen Zeitpunkt vorhanden ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b57314055079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtitle_for_time(time_in_seconds, subs):\n",
    "    for sub in subs:\n",
    "        start_time = sub.start.hours * 3600 + sub.start.minutes * 60 + sub.start.seconds + sub.start.milliseconds / 1000.0\n",
    "        end_time = sub.end.hours * 3600 + sub.end.minutes * 60 + sub.end.seconds + sub.end.milliseconds / 1000.0\n",
    "        if start_time <= time_in_seconds <= end_time:\n",
    "            return sub.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3188d476d1bac",
   "metadata": {},
   "source": [
    "### Funktion: `get_all_frames_cropped`\n",
    "\n",
    "Diese Funktion extrahiert und verarbeitet Frames aus einem Video, wobei sie auf einen zentralen Bereich um den Leuchtturm herum zugeschnitten werden. Optional kann Otsu's Methode zur Bildsegmentierung angewendet werden, wodurch Hintergrundrauschen keinen Einfluss mehr auf die Berechnung und Analyse hat. Dabei werden alle Hintergrundpixel auf 0 gesetzt, wodurch der Hintergrund schwarz wird.\n",
    "\n",
    "### Ablauf:\n",
    "1. **Bounding Boxes analysieren**: Berechnung des zentralen Bereichs für den Crop basierend auf den Bounding Boxes.\n",
    "2. **Crop-Bereich festlegen**: Der Bereich wird auf eine Mindestgröße begrenzt und innerhalb der Bildgrenzen gehalten.\n",
    "3. **Frames verarbeiten**: \n",
    "   - Jedes Frame wird auf den definierten Bereich zugeschnitten.\n",
    "   - Optional: Otsu's Methode zur Trennung von Vorder- und Hintergrund.\n",
    "   - Ermittlung des passenden Untertitels für das aktuelle Frame.\n",
    "4. **Ergebnis**: Gibt die zugeschnittenen Frames und die dazugehörigen Untertitel als Arrays zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3a1744bd656d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_frames_cropped(video_path: str, srt_path: str, bounding_boxes: np.ndarray, height: int, width: int, use_otsu: bool = False) -> np.ndarray:\n",
    "\n",
    "    x_mean = int(np.mean(bounding_boxes[:, 0]) + np.mean(bounding_boxes[:, 2]) // 2)\n",
    "    y_mean = int(np.mean(bounding_boxes[:, 1]) + np.mean(bounding_boxes[:, 3]) // 2)\n",
    "\n",
    "    # Mindestgröße von 200x200px für den Crop-Bereich festlegen\n",
    "    crop_width = max(width, int(np.mean(bounding_boxes[:, 2])))\n",
    "    crop_height = max(height, int(np.mean(bounding_boxes[:, 3])))\n",
    "\n",
    "    # Position so anpassen, dass der Punkt in der Mitte des Crop-Bereichs ist\n",
    "    x_start = max(0, x_mean - crop_width // 2)\n",
    "    y_start = max(0, y_mean - crop_height // 2)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    subs = pysrt.open(srt_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  \n",
    "      \n",
    "    # Sicherstellen, dass der Crop-Bereich innerhalb des Bildes liegt\n",
    "    if x_start + crop_width > frameWidth:\n",
    "        x_start = frameWidth - crop_width\n",
    "    if y_start + crop_height > frameHeight:\n",
    "        y_start = frameHeight - crop_height\n",
    "\n",
    "    subtitles = [None] * frameCount\n",
    "\n",
    "    buf = np.empty((frameCount, height, width, 3), np.dtype('uint8'))\n",
    "    fc = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        # Zuschneiden des Frames auf den zentrierten Bereich\n",
    "        cropped_frame = frame[y_start:y_start+crop_height, x_start:x_start+crop_width]\n",
    "\n",
    "        frame_time = fc / fps\n",
    "\n",
    "        subtitle = get_subtitle_for_time(frame_time, subs)\n",
    "        subtitles[fc] = subtitle\n",
    "\n",
    "        if use_otsu:\n",
    "            # Otsu's Methode anwenden, um Vorder- und Hintergrund zu trennen\n",
    "            gray = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Erstellen des binären Bildes (Vordergrund = 1, Hintergrund = 0)\n",
    "            binary_mask = mask // 255  # Umwandlung in 0 und 1\n",
    "            # Anwenden der Maske auf das Originalbild, um den Hintergrund zu schwärzen\n",
    "            filtered_frame = cropped_frame * np.expand_dims(binary_mask, axis=-1)\n",
    "\n",
    "        else:\n",
    "            filtered_frame = cropped_frame\n",
    "\n",
    "        buf[fc] = cv2.cvtColor(filtered_frame, cv2.COLOR_BGR2RGB)\n",
    "        fc += 1\n",
    "\n",
    "    return buf, subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebffff8c3f239be",
   "metadata": {},
   "source": [
    "Aufrufen der vorher definierten Methode und speichern in eine lokale Variabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204beac455d95561",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = get_Bounding_Boxes(video_path)\n",
    "frames_cropped, subtitles = get_all_frames_cropped(video_path, srt_path, np.array(bounding_boxes), 400, 400, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b79a705e5f546",
   "metadata": {},
   "source": [
    "### Funktionen: `get_avg_hsv` und `get_avg_rgb`\n",
    "\n",
    "Diese Funktionen berechnen den durchschnittlichen Farbwert eines Bildes im HSV- bzw. RGB-Farbraum.\n",
    "\n",
    "#### `get_avg_hsv`:\n",
    "1. **RGB zu HSV konvertieren**: Wandelt das Bild in den HSV-Farbraum um.\n",
    "2. **Maske anwenden**: Verwendet eine Maske, um gültige Pixel auszuwählen.\n",
    "3. **Ergebnis**: Gibt den durchschnittlichen HSV-Wert des Bildes zurück.\n",
    "\n",
    "#### `get_avg_rgb`:\n",
    "1. **Maske anwenden**: Erzeugt eine Maske, um schwarze Pixel auszuschließen.\n",
    "2. **Ergebnis**: Gibt den durchschnittlichen RGB-Wert des Bildes zurück.\n",
    "\n",
    "#### Iteration:\n",
    "- **Averages berechnen**: Iteriert über alle Frames und berechnet die durchschnittlichen HSV- und RGB-Werte für jedes Frame, speichert diese in den Arrays `averages_hsv` und `averages_rgb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76217a269d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_hsv(rgb_image):\n",
    "    hsv_image = rgb_image.astype('float32')/255\n",
    "    hsv_image = cv2.cvtColor(hsv_image, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    lower_limit = np.array([0, 0, 0])\n",
    "    upper_limit = np.array([360, 255, 255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv_image, lower_limit, upper_limit)\n",
    "    \n",
    "    return cv2.mean(hsv_image, mask)[:3]\n",
    "\n",
    "def get_avg_rgb(rgb_image):\n",
    "    mask = cv2.inRange(rgb_image, np.array([1, 1, 1]), np.array([255, 255, 255]))\n",
    "    return cv2.mean(rgb_image, mask)[:3]\n",
    "\n",
    "\n",
    "averages_hsv = np.zeros((frames_cropped.shape[0], frames_cropped.shape[3]), np.dtype('float32'))\n",
    "averages_rgb = averages_hsv.copy()\n",
    "for i in range(frames_cropped.shape[0]):\n",
    "    averages_hsv[i] = get_avg_hsv(frames_cropped[i])\n",
    "    averages_rgb[i] = get_avg_rgb(frames_cropped[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640045158c9e0466",
   "metadata": {},
   "source": [
    "### Funktion: `getIndicies`\n",
    "\n",
    "Diese Funktion ermittelt die Indizes der größten Gradienten also der größten absoluten Steigung in einem gegebenen Array. Dabei wird für jeden Kanal ein eigener Index berechnet und diese werden als Array zurückgegeben.\n",
    "\n",
    "#### Ablauf:\n",
    "1. **Gradienten berechnen**: Für jede der drei Spalten des `averages`-Arrays werden die Gradienten über eine festgelegte Anzahl von Schritten berechnet.\n",
    "2. **Abfall identifizieren**: Der Index des größten Abfalls (bzw. des steilsten Anstiegs) in den Gradienten wird für jede Spalte bestimmt.\n",
    "3. **Ergebnis**: Gibt eine Liste der Indizes mit den steilsten Gradienten für jede Spalte zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c0c242ee29784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndices(averages: np.ndarray) -> list:\n",
    "    indices = []\n",
    "    for i in range(3):\n",
    "        n = 2 \n",
    "        gradients = averages[n:, i] - averages[:-n, i]\n",
    "        x = np.min(gradients)\n",
    "\n",
    "        # Index des steilsten Abfalls finden\n",
    "        x_index = np.argmax( np.abs(gradients))\n",
    "        indices.append(x_index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c0429895eec2d",
   "metadata": {},
   "source": [
    "### Funktion: `plot`\n",
    "\n",
    "Diese Funktion visualisiert die Durchschnittswerte eines Arrays und markiert die bestimmten Indizes in den Diagrammen.\n",
    "\n",
    "#### Ablauf:\n",
    "1. **Subplots erstellen**: Erstellt drei separate Diagramme (eines für jede Spalte des `averages`-Arrays).\n",
    "2. **Indizes markieren**: Zeichnet eine vertikale Linie an den angegebenen Indizes.\n",
    "3. **Daten plotten**: Plottet die Durchschnittswerte der drei Spalten.\n",
    "4. **Anzeigen**: Zeigt die Plots an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a97dd09090ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(averages: np.ndarray, indices: list):\n",
    "\n",
    "    fig, axs = plt.subplots(3, figsize=(8, 8))\n",
    "\n",
    "    print(indices)\n",
    "\n",
    "    axs[0].axvline(x = indices[0], color = 'b')\n",
    "    axs[0].plot(averages[:, 0])\n",
    "    axs[1].axvline(x = indices[1], color = 'b')\n",
    "    axs[1].plot(averages[:, 1])\n",
    "    axs[2].axvline(x = indices[2], color = 'b')\n",
    "    axs[2].plot(averages[:, 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa1ef52e36586df",
   "metadata": {},
   "source": [
    "Aufrufen der `plot` Methoden mit jeweils HSV-Werten und RGB-Werten. Dabei werden die jeweiligen Diagramme ausgegeben und die berechneten Indizes ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857180118cb2b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(averages_hsv, getIndices(averages_hsv))\n",
    "plot(averages_rgb, getIndices(averages_rgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36023c08d6dfef0e",
   "metadata": {},
   "source": [
    "### Code-Beschreibung\n",
    "\n",
    "Dieser Code bestimmt den Wechselrahmen basierend auf durchschnittlichen Farbwerten und wählt das entsprechende Frame aus.\n",
    "\n",
    "#### Ablauf:\n",
    "1. **Indizes berechnen**: Die Funktion `getIndices` wird auf die durchschnittlichen RGB-Werte (`averages_rgb`) angewendet, um relevante Indizes zu ermitteln.\n",
    "2. **Wechselrahmen bestimmen**: Berechnet den Mittelwert der Indizes und rundet ihn, um den Wechselrahmen (`switch_frame`) zu bestimmen.\n",
    "3. **Frame auswählen**: Das Frame an der Position `switch_frame` wird aus `frames_cropped` extrahiert und als `frame` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe4e06114348f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch_frame = int(np.round(np.mean(getIndices(averages_hsv))))\n",
    "switch_frame = int(np.round(np.mean(getIndices(averages_rgb))))\n",
    "frame = frames_cropped[switch_frame]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717abbe15a6b06be",
   "metadata": {},
   "source": [
    "### Code-Beschreibung\n",
    "\n",
    "Dieser Code zeigt das vorher bestimmte Bild an und fügt den Untertitel mit geografischen Koordinaten hinzu.\n",
    "\n",
    "#### Ablauf:\n",
    "1. **Bild anzeigen**: Erstellt eine Plotfigur und zeigt das ausgewählte Frame (`frame`) an.\n",
    "2. **Untertitel extrahieren**: Extrahiert die geografischen Koordinaten (Breitengrad und Längengrad) aus dem Untertitel des `switch_frame`.\n",
    "3. **Text hinzufügen**: Platziert den Untertitel mit den Koordinaten auf dem Bild, wobei der Text weiß ist und einen schwarzen Hintergrund hat.\n",
    "4. **Anzeigen**: Zeigt das Bild mit dem überlagerten Text an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e3d4be73ec3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.imshow(frame)\n",
    "subtitle = str(subtitles[switch_frame]).split('[')\n",
    "subtitle = [subtitle[9].split(\"latitude: \")[1][:~1], subtitle[10].split(\"longitude: \")[1][:~1]]\n",
    "plt.text(5, 10, subtitle, color='white', fontsize=12, bbox=dict(facecolor='black', alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79067161b81be11",
   "metadata": {},
   "source": [
    "### Funktion: `generate_video_with_subtitles`\n",
    "\n",
    "Diese Funktion erstellt ein neues Video mit eingebetteten Metadaten und markierten Bounding Boxes. Dabei werden, sobald ein Wechsel der Farben bestimmt wurde, zusätzlich die Metadaten des bestimmten Wechselzeitpunkt-Bildes angezeigt. Die Bounding-Boxes ändern dabei ebenfalls die Farbe. \n",
    "\n",
    "#### Ablauf:\n",
    "1. **Video einlesen**: Lädt das Eingabevideo und ermittelt die Eigenschaften (Breite, Höhe, FPS).\n",
    "2. **VideoWriter initialisieren**: Bereitet den VideoWriter vor, um das bearbeitete Video zu speichern.\n",
    "3. **Frames bearbeiten**:\n",
    "   - Fügt Untertitel zu jedem Frame hinzu.\n",
    "   - Markiert die Bounding Boxes..\n",
    "   - Bei Erreichen des `switch_frame_number` wird der zusätzlicher Text eingeblendet.\n",
    "4. **Video speichern**: Speichert die bearbeiteten Frames im Ausgabefile.\n",
    "5. **Ressourcen freigeben**: Schließt die Video-Dateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea2c96b98d62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_with_subtitles(input_video_path, output_video_path, switch_frame_number):\n",
    "    # Video einlesen\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Videoeigenschaften ermitteln\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # VideoWriter zum Speichern des neuen Videos initialisieren\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Video durchlaufen und bearbeiten\n",
    "    frame_counter = 0\n",
    "    switch_subtitle = str(subtitles[switch_frame_number+1]).split('[')\n",
    "    switch_subtitle = [switch_subtitle[9].split(\"latitude: \")[1][:~1], switch_subtitle[10].split(\"longitude: \")[1][:~1]]\n",
    "\n",
    "    while cap.isOpened():\n",
    "        try:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            subtitle = str(subtitles[frame_counter+1]).split('[')\n",
    "            subtitle = [subtitle[9].split(\"latitude: \")[1][:~1], subtitle[10].split(\"longitude: \")[1][:~1]]\n",
    "\n",
    "            # Untertitel hinzufügen\n",
    "            text_position = (50, 100)\n",
    "            cv2.putText(frame, f\"Lat: {subtitle[0]}, Long: {subtitle[1]}\", text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA) \n",
    "\n",
    "            if frame_counter >= switch_frame_number:\n",
    "                cv2.putText(frame, f\"GEAENDERT bei: Frame: {switch_frame_number}, Lat: {switch_subtitle[0]}, Long: {switch_subtitle[1]}\", (50,150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                x, y, w, h = bounding_boxes[frame_counter+1] \n",
    "                cv2.rectangle(frame, (x + w//4, y + h//4), (x + w//4 + w, y + h//4 + h), (0, 0, 255), 2)\n",
    "            else:\n",
    "\n",
    "                x, y, w, h = bounding_boxes[frame_counter+1] \n",
    "                cv2.rectangle(frame, (x + w//4, y + h//4), (x + w//4 + w, y + h//4 + h), (0, 255, 0), 2)\n",
    "        \n",
    "\n",
    "            # Frame speichern\n",
    "            out.write(frame)\n",
    "            frame_counter += 1\n",
    "        except Exception as ex:\n",
    "            pass\n",
    "    # Alles freigeben\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Beispielaufruf\n",
    "generate_video_with_subtitles(video_path, output_video, switch_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7471087de4d3f",
   "metadata": {},
   "source": [
    "### Prüfung des Kurses\n",
    "\n",
    "Zur Überprüfung des Kurses werden die Formeln zur Besteckrechnung nach Mittelbreite aus der Nautischen Formelsammlung Navigation[[6]](https://www.rolfdreyer.de/downloads/Formelsammlung.pdf) von Prof. Werner Huth verwendet. Der Kurs berechnet sich wie folgt: \n",
    "\n",
    "$$ tan(a) = \\frac{\\Delta\\lambda \\times \\cos(\\varphi_m)}{\\Delta\\varphi} $$\n",
    "\n",
    "Wobei $ \\Delta\\lambda $ den Längenunterschied $ \\lambda_B - \\lambda_A $ und $ \\Delta\\varphi $ den Breitenunterschied $ \\varphi_B - \\varphi_A $ darstellen. Die mittlere Breite $ \\varphi_m $ berechnet sich folgendermaßen:\n",
    "\n",
    "$$ \\varphi_m = \\frac{1}{2} \\times (\\varphi_A + \\varphi_B) $$\n",
    "\n",
    "In den folgenden Code-Zellen werden diese implementiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5ad93440c740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mittlere_breite(breite_a, breite_b):\n",
    "    return 0.5 * (breite_a + breite_b)\n",
    "\n",
    "def kurs_berechnen(laenge_a, laenge_b, breite_a, breite_b):\n",
    "    breite_a = math.radians(float(breite_a))\n",
    "    breite_b = math.radians(float(breite_b))\n",
    "    laenge_a = math.radians(float(laenge_a))\n",
    "    laenge_b = math.radians(float(laenge_b))\n",
    "    \n",
    "    laengen_unterschied = laenge_b - laenge_a\n",
    "    breiten_unterschied = breite_b - breite_a\n",
    "    \n",
    "    kurs = math.atan2(\n",
    "        laengen_unterschied * math.cos(mittlere_breite(breite_a, breite_b)),\n",
    "        breiten_unterschied\n",
    "    )\n",
    "\n",
    "    kurs = math.degrees(kurs)\n",
    "    \n",
    "    if kurs < 0:\n",
    "        kurs += 360\n",
    "        \n",
    "    return kurs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afba7781bdf89d4",
   "metadata": {},
   "source": [
    "Die für die Berechnung des Kurses benötigten Werte werden in der folgenden Zelle in ein Array von Dictionaries zusammengefasst. Ein Eintrag in diesem Array entspricht einen Videoausschnitt. Zu diesen Werten gehören der Name des Videos, der Untertitel in dem die Positionsdaten hinterlegt sind, die Frame-Indices der potenziellen Sektorengrenzen, sowie der Soll-Kurs der Sektorgrenze. Für eine weitere Evaluation liegen die Durchschnittswerte der Farbkanäle mit bei. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f640ca7f5c71e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoData = []\n",
    "\n",
    "for i in range(1, 7):\n",
    "    if i == 6:\n",
    "        flug = 2\n",
    "        part = 1\n",
    "    else:\n",
    "        flug = 1\n",
    "        part = i\n",
    "    \n",
    "    kurs = 24    \n",
    "    \n",
    "    if i == 5:\n",
    "        kurs = 320\n",
    "    \n",
    "    path = \"data/Flug_\" + str(flug) + \"_part_\" + str(part)\n",
    "\n",
    "    bounding_boxes = get_Bounding_Boxes(path + \".MP4\")\n",
    "    frames_cropped, subtitles = get_all_frames_cropped(path + \".MP4\", path + \".SRT\", np.array(bounding_boxes), 400, 400, True)\n",
    "    \n",
    "    averages_hsv = np.zeros((frames_cropped.shape[0], frames_cropped.shape[3]), np.dtype('float32'))\n",
    "    averages_rgb = averages_hsv.copy()\n",
    "    \n",
    "    for i in range(frames_cropped.shape[0]):\n",
    "        averages_hsv[i] = get_avg_hsv(frames_cropped[i])\n",
    "        averages_rgb[i] = get_avg_rgb(frames_cropped[i])\n",
    "\n",
    "    indices_hsv = getIndices(averages_hsv)\n",
    "    indices_rgb = getIndices(averages_rgb)\n",
    "    \n",
    "    videoData.append({\n",
    "        \"name\": \"Flug \" + str(flug) + \" Part \" + str(part),\n",
    "        \"subtitles\": subtitles,\n",
    "        \"averages_hsv\": averages_hsv,\n",
    "        \"averages_rgb\": averages_rgb,\n",
    "        \"indices_hsv\": indices_hsv,\n",
    "        \"indices_rgb\": indices_rgb,\n",
    "        \"kurs_soll\": kurs\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed731c34549e10",
   "metadata": {},
   "source": [
    "Für die Berechnung des Kurses und der Entfernung zum Sektorenfeuer ist zunächst eine Umrechnung der vorliegenden Breiten- und Längengraden des Sektorenfeuers von ganzzahligen Grad, Minuten und Sekunden in Grad als Dezimalzahl erforderlich. Aus dem Untertitel des Frames, in dem die Sektorgrenze zu erkennen ist, werden die Breiten- und Längengrade der Drohne entnommen. Anhand der Position der Drohne und des Sektorenfeuers wird mittels der Funktion `kurs_berechnen` der Kurs ermittelt. Die Berechnung der Entfernung erfolgt mittels des Pakets `geopy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9c5480ce946b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "breitengrad_sektorenfeuer = 53 + 20/60 + 10/3600\n",
    "laengengrad_sektorenfeuer = 7 + 6/60 + 30/3600\n",
    "\n",
    "for video in videoData:\n",
    "    frame_index_sektorgrenze = getIndices(video['averages_rgb'])[0]\n",
    "    subtitles_frame = video['subtitles'][frame_index_sektorgrenze]\n",
    "\n",
    "    breitengrad_drohne = float(subtitles_frame.split(\"latitude: \")[1][:~1].split(\"]\")[0])\n",
    "    laengengrad_drohne = float(subtitles_frame.split(\"longitude: \")[1][:~1].split(\"]\")[0])\n",
    "\n",
    "    kurs = kurs_berechnen(\n",
    "        laengengrad_drohne, laengengrad_sektorenfeuer,\n",
    "        breitengrad_drohne, breitengrad_sektorenfeuer,\n",
    "    )\n",
    "        \n",
    "    entfernung = distance.distance(\n",
    "        (breitengrad_drohne, laengengrad_drohne), \n",
    "        (breitengrad_sektorenfeuer, laengengrad_sektorenfeuer)\n",
    "    ).nautical\n",
    "\n",
    "    print(f'Video {video[\"name\"]}')\n",
    "    print(f'Position: N {np.round(breitengrad_drohne, 5)}° - E {np.round(laengengrad_drohne, 5)}°, Entfernung: {np.round(entfernung, 3)}sm')\n",
    "    print(f'Der Kurs beträgt {np.round(kurs, 2)}° bei Frame {frame_index_sektorgrenze}, Abweichung von {np.round(kurs - video[\"kurs_soll\"], 2)}°')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e8a60fb95be6a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "| Video         | Erwarteter Kurs | Frame Sektorgrenze | Frame berechnet | Kurs berechnet | Differenz | Entfernung |\n",
    "|---------------|-----------------|--------------------|-----------------|----------------|-----------|------------|\n",
    "| Flug 1 Part 1 | 24°             | 84                 | 83              | 28.55°         | 4.55°     | 0.048sm    |\n",
    "| Flug 1 Part 2 | 24°             | 113                | 111             | 28.77°         | 4.77°     | 0.048sm    |\n",
    "| Flug 1 Part 3 | 24°             | 106                | 105             | 28.85°         | 4.85°     | 0.048sm    |\n",
    "| Flug 1 Part 4 | 24°             | 64                 | 63              | 27.76°         | 3.76°     | 0.114sm    |\n",
    "| Flug 1 Part 5 | 320°            | Nicht zu erkennen  | 86              | 323.05°        | 3.05°     | 0.110sm    |\n",
    "| Flug 2 Part 1 | 24°             | 83                 | 82              | 27.16°         | 3.16°     | 0.213sm    |\n",
    "\n",
    "In der oberen Tabelle sind der erwartete Kurs für die Sektorgrenze, sowie die manuell ermittelten Frames auf denen der Übergang zwischen zwei Sektoren zu erkennen ist. Im fünften Abflug ist kein eindeutiger Übergang zu erkennen, da zum Zeitpunkt des Übergangs die Kamera der Drohne ihre Richtung ändert. Der berechnete Frame 86 befindet sich jedoch in einem Bereich um diesen Zeitpunkt. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./abbildungen/wybelsum_kurs_flug_1_4.png\" width=\"900\">\n",
    "<br>\n",
    "Abbildung 3: Flug 1, Part 4. Eingezeichneter Kurs\n",
    "</p>\n",
    "\n",
    "Der berechnete Kurs weicht vom erwarteten Kurs in etwa um drei bis fünf Grad ab. Die Ursachen für diese Abweichung können in ungenauen Positionsdaten der Drohne liegen, oder auch in einer tatsächlichen Abweichung der Sektorgrenze. Zu erkennen ist, dass die Abweichung sich bei größeren Abständen verringert, was für eine Abweichung in den Positionsdaten der Drohne spricht. Die Abweichung ist in dem in Abbildung 3 eingezeichneten Kurs zu erkennen. Die Differenz zwischen den hier berechneten Kurs und den vom OpenSeaMap berechneten Kurs lässt sich durch Rundungsfehler erklären, da die Position in OpenSeaMap selbst mit ganzen Sekunden angegeben ist.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f387aceda575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hsv_rgb(averages_hsv, indices_hsv, averages_rgb, indices_rgb, title):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    axs[0][0].axvline(x = indices_hsv[0], color = 'b')\n",
    "    axs[0][0].plot(averages_hsv[:, 0])\n",
    "    axs[0][0].set_title(\"H Kanal\")\n",
    "    axs[0][0].set_xlabel(\"Frame\")\n",
    "    axs[1][0].axvline(x = indices_hsv[1], color = 'b')\n",
    "    axs[1][0].plot(averages_hsv[:, 1])\n",
    "    axs[1][0].set_title(\"S Kanal\")\n",
    "    axs[1][0].set_xlabel(\"Frame\")\n",
    "    axs[2][0].axvline(x = indices_hsv[2], color = 'b')\n",
    "    axs[2][0].plot(averages_hsv[:, 2])\n",
    "    axs[2][0].set_title(\"V Kanal\")\n",
    "    axs[2][0].set_xlabel(\"Frame\")\n",
    "\n",
    "    axs[0][1].axvline(x = indices_rgb[0], color = 'b')\n",
    "    axs[0][1].plot(averages_rgb[:, 0])\n",
    "    axs[0][1].set_title(\"Rot\")\n",
    "    axs[0][1].set_xlabel(\"Frame\")\n",
    "    axs[1][1].axvline(x = indices_rgb[1], color = 'b')\n",
    "    axs[1][1].plot(averages_rgb[:, 1])\n",
    "    axs[1][1].set_title(\"Grün\")\n",
    "    axs[1][1].set_xlabel(\"Frame\")\n",
    "    axs[2][1].axvline(x = indices_rgb[2], color = 'b')\n",
    "    axs[2][1].plot(averages_rgb[:, 2])\n",
    "    axs[2][1].set_title(\"Blau\")\n",
    "    axs[2][1].set_xlabel(\"Frame\")\n",
    "    plt.show()\n",
    "\n",
    "for video in videoData:\n",
    "    plot_hsv_rgb(video['averages_hsv'], video['indices_hsv'], video['averages_rgb'], video['indices_rgb'], video['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ddd740f91ec5a",
   "metadata": {},
   "source": [
    "Die oben gezeigten Diagramme zeigen den Verlauf der einzelnen Farbkanäle über die Zeit. Für eine Grenze zwischen roten und weißen Sektoren lassen sich mit den Farbkanälen H und S im HSV-Farbraum sowie Grün im RGB-Farbraum abweichende Indices für den Ziel-Frame ermitteln. Ebenso ist zu erkennen, dass im fünften Ausschnitt des ersten Fluges kein Erfassen des Leuchtfeuers erfolgt. Nach manueller Sichtung des Ausschnitts liegt der Grund hierfür, dass die Rücklichter eines parkenden Autos ebenso erfasst werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed913121818fdcce",
   "metadata": {},
   "source": [
    "## Fazit\n",
    "\n",
    "Durch die erfolgte Bestimmung von Sektorgrenzen mittels Drohnenaufnahmen ist der Abschluss dieses Projekts als erfolgreich zu sehen. Das Projekt zeigt mögliche Implementierungen zur Bestimmung der Position des Sektorenfeuers in einem Video sowie den Zeitpunkt des Wechsels zwischen zwei Sektoren. Anhand der im Untertitel beiliegenden Metadaten lässt sich die Position der Drohne bestimmen, um somit den Kurs zu berechnen und diesen mit dem Erwartungswert gegenüberzustellen. Die Genauigkeit der Daten ist aus Sicht der Autoren den Umständen entsprechend zufriedenstellend. Möglichkeiten zur verbesserung der Messwerte liegen in der Verwendung eines genaueren GPS-Moduls für die Drohne, sowie eine Vergrößerung des Abstands zwischen der Drohne und des Sektorenfeuers. Für die Verwendung der in dieser Arbeit gezeigten Methoden bedarf es allerdings noch weitere Feinabstimmungen in der Erfassung des Leuchtfeuers. Neben der Verbesserung der Ergebnisse können zukünftige Projekte, welche auf diese Arbeit aufbauen, die Erfassung und Übergänge andersfarbiger Sektoren sowie Sektoren mit unterschiedlichen Kennungen, wie beispielsweise Blinkfeuer. \n",
    "\n",
    "## Literaturverzeichnis\n",
    "\n",
    "- [1] Sportbootführerschein See kompakt https://doi.org/10.24053/9783739882161\n",
    "- [2] Deutsche Leuchtfeuer: Leuchtturm Campen https://www.deutsche-leuchtfeuer.de/nordsee/campen.html\n",
    "- [3] Deutsche Leuchtfeuer: Leuchtfeuer Wybelsum https://www.deutsche-leuchtfeuer.de/binnen/ems/wybelsum.html\n",
    "- [4] OpenSeaMap https://map.openseamap.org/\n",
    "- [5] Traffic light detection with color and edge information https://ieeexplore.ieee.org/document/5234518\n",
    "- [6] Nautischen Formelsammlung Navigation https://www.rolfdreyer.de/downloads/Formelsammlung.pdf\n",
    "- [7] Influence of normalization and color space to color texture classification https://www.sciencedirect.com/science/article/pii/S0031320316301510\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
